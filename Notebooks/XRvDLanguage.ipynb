{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3xaJ8--FlHj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from wordcloud import WordCloud\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "final_merged = pd.read_csv('/content/drive/MyDrive/Applied Deep Learning Poli Sci/Foley and Dorner Repo/Data/final_merged.csv')\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "model = Word2Vec.load('/content/drive/MyDrive/Applied Deep Learning Poli Sci/Foley and Dorner Repo/Models/w2v100_tweets_model.model')\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stopwords I added after first iteration\n",
        "additional_stopwords = {'amp', 'QT', 'RT'}\n",
        "\n",
        "#somehow, I messed up removing the hashtags, don't have time to fix. the resulting word clouds are basically identical due to this\n",
        "def clean_and_tokenize(tweet):\n",
        "    #removes urls\n",
        "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
        "    #removes file extensions\n",
        "    tweet = re.sub(r'\\S+\\.(jpg|jpeg|png|gif|mp4|vid|video|html|htm)', '', tweet)\n",
        "    #removes some strings\n",
        "    tweet = re.sub(r'\\b(twitter|twimg|media|480x270)\\b', '', tweet, flags=re.IGNORECASE)\n",
        "    #tokenizes\n",
        "    tokens = word_tokenize(tweet)\n",
        "    #removes stopwords\n",
        "    tokens = [word for word in tokens if word.lower() not in stopwords.words('english') and word.lower() not in additional_stopwords]\n",
        "    #removes non-alphanumeric tokens\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "    return tokens\n",
        "\n",
        "def tokenize_hashtags(tweet):\n",
        "    #extracts hashtags\n",
        "    hashtags = re.findall(r'#\\w+', tweet)\n",
        "    #removes the '#' symbol\n",
        "    hashtags = [hashtag[1:] for hashtag in hashtags]\n",
        "    #removes stopwords\n",
        "    hashtags = [word for word in hashtags if word.lower() not in stopwords.words('english') and word.lower() not in additional_stopwords]\n",
        "    return hashtags\n",
        "\n",
        "#runs the functions\n",
        "final_merged['cleaned_tokens'] = final_merged['text'].apply(clean_and_tokenize)\n",
        "final_merged['hashtags'] = final_merged['text'].apply(tokenize_hashtags)\n",
        "\n",
        "print(final_merged[['text', 'cleaned_tokens', 'hashtags']])"
      ],
      "metadata": {
        "id": "elmHsCiSLnzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generates word clouds, sorry for lack of comments, doing this in a pinch\n",
        "def plot_word_cloud(text, title, max_words=50):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "republican_words = set([word for tokens in final_merged[final_merged['party'] == 'R']['cleaned_tokens'] for word in tokens])\n",
        "democrat_words = set([word for tokens in final_merged[final_merged['party'] == 'D']['cleaned_tokens'] for word in tokens])\n",
        "unique_republican_words = republican_words - democrat_words\n",
        "unique_democrat_words = democrat_words - republican_words\n",
        "\n",
        "republican_text = ' '.join([' '.join(tokens) for tokens in final_merged[final_merged['party'] == 'R']['cleaned_tokens']])\n",
        "democrat_text = ' '.join([' '.join(tokens) for tokens in final_merged[final_merged['party'] == 'D']['cleaned_tokens']])\n",
        "unique_republican_text = ' '.join(unique_republican_words)\n",
        "unique_democrat_text = ' '.join(unique_democrat_words)\n",
        "\n",
        "plot_word_cloud(unique_republican_text, 'Unique Words in Republican Tweets')\n",
        "plot_word_cloud(unique_democrat_text, 'Unique Words in Democrat Tweets')\n",
        "\n",
        "republican_hashtags = set([hashtag for hashtags in final_merged[final_merged['party'] == 'R']['hashtags'] for hashtag in hashtags])\n",
        "democrat_hashtags = set([hashtag for hashtags in final_merged[final_merged['party'] == 'D']['hashtags'] for hashtag in hashtags])\n",
        "unique_republican_hashtags = republican_hashtags - democrat_hashtags\n",
        "unique_democrat_hashtags = democrat_hashtags - republican_hashtags\n",
        "\n",
        "republican_hashtag_text = ' '.join([' '.join(hashtags) for hashtags in final_merged[final_merged['party'] == 'R']['hashtags']])\n",
        "democrat_hashtag_text = ' '.join([' '.join(hashtags) for hashtags in final_merged[final_merged['party'] == 'D']['hashtags']])\n",
        "unique_republican_hashtag_text = ' '.join(unique_republican_hashtags)\n",
        "unique_democrat_hashtag_text = ' '.join(unique_democrat_hashtags)\n",
        "\n",
        "plot_word_cloud(unique_republican_hashtag_text, 'Unique Hashtags in Republican Tweets')\n",
        "plot_word_cloud(unique_democrat_hashtag_text, 'Unique Hashtags in Democrat Tweets')"
      ],
      "metadata": {
        "id": "37Pb1PgOGtyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "#separates the tweets by party\n",
        "republican_tweets = final_merged[final_merged['party'] == 'R']['cleaned_tokens']\n",
        "democrat_tweets = final_merged[final_merged['party'] == 'D']['cleaned_tokens']\n",
        "\n",
        "#creates a dictionary and corpus for each group\n",
        "def create_corpus_and_dictionary(tokenized_texts):\n",
        "    dictionary = corpora.Dictionary(tokenized_texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
        "    return corpus, dictionary\n",
        "\n",
        "republican_corpus, republican_dictionary = create_corpus_and_dictionary(republican_tweets)\n",
        "democrat_corpus, democrat_dictionary = create_corpus_and_dictionary(democrat_tweets)\n",
        "\n",
        "#trains the LDA models\n",
        "num_topics = 10\n",
        "republican_lda_model = LdaModel(corpus=republican_corpus, id2word=republican_dictionary, num_topics=num_topics, random_state=42)\n",
        "democrat_lda_model = LdaModel(corpus=democrat_corpus, id2word=democrat_dictionary, num_topics=num_topics, random_state=42)"
      ],
      "metadata": {
        "id": "ZTrwlYKrdTrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that displays the topics\n",
        "def display_topics(model, num_words):\n",
        "    topics = model.print_topics(num_topics=num_topics, num_words=num_words)\n",
        "    for topic_num, topic in topics:\n",
        "        topic_words = topic.split(' + ')\n",
        "        topic_words = [word.split('*')[1].strip('\"') for word in topic_words]\n",
        "        print(f'Topic {topic_num}: {\" \".join(topic_words)}')\n",
        "\n",
        "print(\"Republican Topics:\")\n",
        "display_topics(republican_lda_model, 15)\n",
        "print(\"\\nDemocrat Topics:\")\n",
        "display_topics(democrat_lda_model, 15)"
      ],
      "metadata": {
        "id": "V-3CvlqDdadW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
