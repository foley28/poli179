{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62dbd144",
      "metadata": {
        "id": "62dbd144",
        "outputId": "ca018ea4-4ef4-4fec-d27f-e12ba8fdf6e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: datasets in /opt/conda/lib/python3.9/site-packages (2.18.0)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (2024.2.0)\n",
            "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (5.4.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (2.26.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.9/site-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.9/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets) (2.2.0)\n",
            "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets) (3.9.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets) (2021.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (0.28.0)\n",
            "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate) (5.4.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.9/site-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate) (5.8.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from accelerate) (2.1.2+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.9/site-packages (from accelerate) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.8)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.0.1)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.26.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.9/site-packages (from networkx->torch>=1.10.0->accelerate) (5.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.6)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-06-05 20:13:35.902577: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformers version: 4.39.2\n",
            "Accelerate version: 0.28.0\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Install required libraries\n",
        "!pip install datasets\n",
        "!pip install accelerate\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Verify versions\n",
        "import transformers\n",
        "import accelerate\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e589a4e2",
      "metadata": {
        "id": "e589a4e2",
        "outputId": "792e74f4-05cb-4d68-f6de-3d989d05c228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  nominate_dim1\n",
            "0  E-Alert: Rogers: House-Passed NDAA Supports ou...          0.377\n",
            "1  eAlert: Rogers Praises Supreme Court's Decisio...          0.377\n",
            "2  eAlert: Rogers Reintroduces Legislation to Pro...          0.377\n",
            "3   We Must Secure our Border to Keep Americans Safe          0.377\n",
            "4  eNewsletter: Biden's Socialist Policies are Hu...          0.377\n"
          ]
        }
      ],
      "source": [
        "'''from google.colab import drive\n",
        "import shutil\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/My Drive/final_jawn.csv'\n",
        "destination_path = '/content/sample_data/'\n",
        "shutil.copy(file_path, destination_path)'''\n",
        "\n",
        "df = pd.read_csv('/home/rfoley/private/JAWNDCI.csv')\n",
        "\n",
        "# Assuming the DataFrame has columns 'text' and 'nominate_dim1' for the ideological scores\n",
        "# Select only the required columns\n",
        "sdf = df[['Subject', 'nominate_dim1']]\n",
        "sdf.columns = ['text','nominate_dim1']\n",
        "print(sdf.head())\n",
        "bdf = df[['Body', 'nominate_dim1']]\n",
        "bdf.columns = ['text', 'nominate_dim1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4fcf8f",
      "metadata": {
        "id": "ca4fcf8f",
        "outputId": "8c3eac05-a983-4488-c32d-2eb03120dbac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  nominate_dim1\n",
            "0  E-Alert: Rogers: House-Passed NDAA Supports ou...          0.377\n",
            "1  eAlert: Rogers Praises Supreme Court's Decisio...          0.377\n",
            "2  eAlert: Rogers Reintroduces Legislation to Pro...          0.377\n",
            "3   We Must Secure our Border to Keep Americans Safe          0.377\n",
            "4  eNewsletter: Biden's Socialist Policies are Hu...          0.377\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "if sdf.isnull().values.any():\n",
        "    print(\"Warning: Missing values detected in the dataset. Removing missing values...\")\n",
        "    sdf = sdf.dropna()\n",
        "print(sdf.head())\n",
        "if bdf.isnull().values.any():\n",
        "    print(\"Warning: Missing values detected in the dataset. Removing missing values...\")\n",
        "    bdf = bdf.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e42209",
      "metadata": {
        "id": "93e42209"
      },
      "outputs": [],
      "source": [
        "# Convert the DataFrame to a Hugging Face Dataset\n",
        "sdataset = Dataset.from_pandas(sdf)\n",
        "bdataset = Dataset.from_pandas(bdf)\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "sdataset = sdataset.train_test_split(test_size=0.2)\n",
        "s_train_dataset = sdataset['train']\n",
        "s_eval_dataset = sdataset['test']\n",
        "\n",
        "bdataset = bdataset.train_test_split(test_size=0.2)\n",
        "b_train_dataset = bdataset['train']\n",
        "b_eval_dataset = bdataset['test']\n",
        "\n",
        "# Load Roberta tokenizer\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0b56bf4",
      "metadata": {
        "id": "a0b56bf4"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    encoded = roberta_tokenizer(examples['text'], is_split_into_words=False, padding='max_length', truncation=True)\n",
        "    return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "034561eb",
      "metadata": {
        "scrolled": true,
        "id": "034561eb",
        "outputId": "873482a6-3b2d-4526-cb44-fc1b99f768c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'nominate_dim1', '__index_level_0__'],\n",
            "    num_rows: 99916\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(s_train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf23a06",
      "metadata": {
        "id": "9bf23a06",
        "outputId": "389ef8d8-8ddc-4c79-9294-de713c445cf0",
        "colab": {
          "referenced_widgets": [
            "a0e8fee9085c4e519b0d03eb84f297e6",
            "a620fe12b51a4c90bae6d73d5c0e7ed5",
            "1355becd96c54fdc9aac361ff8d32907",
            "47c9a68b527947b886086baa987cfe2c"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0e8fee9085c4e519b0d03eb84f297e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/99916 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a620fe12b51a4c90bae6d73d5c0e7ed5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/24979 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1355becd96c54fdc9aac361ff8d32907",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/99916 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47c9a68b527947b886086baa987cfe2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/24979 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Apply tokenization to the datasets\n",
        "s_train_dataset = s_train_dataset.map(tokenize_function, batched=True)\n",
        "s_eval_dataset = s_eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch tensors\n",
        "s_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'nominate_dim1'])\n",
        "s_eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'nominate_dim1'])\n",
        "\n",
        "# Rename 'nominate_dim1' to 'labels' because Trainer expects labels column\n",
        "s_train_dataset = s_train_dataset.rename_column(\"nominate_dim1\", \"labels\")\n",
        "s_eval_dataset = s_eval_dataset.rename_column(\"nominate_dim1\", \"labels\")\n",
        "\n",
        "# Ensure dataset columns are in the correct format for the model\n",
        "s_train_dataset = s_train_dataset.map(lambda examples: {'labels': examples['labels'].float()}, batched=True)\n",
        "s_eval_dataset = s_eval_dataset.map(lambda examples: {'labels': examples['labels'].float()}, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9627395d",
      "metadata": {
        "id": "9627395d",
        "outputId": "e157e59f-1ca4-46ef-9cbd-7996b91a98f0",
        "colab": {
          "referenced_widgets": [
            "fa7ddc9abeee45f49d6e795f9bf4e7dd",
            "c0ac541f163a42de8affb8fe2a3a415b",
            "b993b03dfce544f681559875697cc22a",
            "3a167ef1064a4ce992d2c768c6e7cc38"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa7ddc9abeee45f49d6e795f9bf4e7dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/99916 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0ac541f163a42de8affb8fe2a3a415b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/24979 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b993b03dfce544f681559875697cc22a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/99916 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a167ef1064a4ce992d2c768c6e7cc38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/24979 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Apply tokenization to the datasets\n",
        "b_train_dataset = b_train_dataset.map(tokenize_function, batched=True)\n",
        "b_eval_dataset = b_eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch tensors\n",
        "b_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'nominate_dim1'])\n",
        "b_eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'nominate_dim1'])\n",
        "\n",
        "# Rename 'nominate_dim1' to 'labels' because Trainer expects labels column\n",
        "b_train_dataset = b_train_dataset.rename_column(\"nominate_dim1\", \"labels\")\n",
        "b_eval_dataset = b_eval_dataset.rename_column(\"nominate_dim1\", \"labels\")\n",
        "\n",
        "# Ensure dataset columns are in the correct format for the model\n",
        "b_train_dataset = b_train_dataset.map(lambda examples: {'labels': examples['labels'].float()}, batched=True)\n",
        "b_eval_dataset = b_eval_dataset.map(lambda examples: {'labels': examples['labels'].float()}, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d60e3eb",
      "metadata": {
        "id": "9d60e3eb",
        "outputId": "c90106ea-6993-4bd0-9852-10efd5ec8e02",
        "colab": {
          "referenced_widgets": [
            "6686c0e70d674f90970464d29af39868"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6686c0e70d674f90970464d29af39868",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/opt/conda/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
            "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "# Define training arguments\n",
        "#https://www.google.com/search?q=how+to+use+less+disk+space+when+training+a+pretrained+model&oq=how+to+use+less+disk+space+when+training+a+pretrained+model&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigATIHCAIQIRigATIHCAMQIRigATIHCAQQIRigATIHCAUQIRirAtIBCTIwOTE4ajBqNKgCALACAQ&sourceid=chrome&ie=UTF-8\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',  # Use 'evaluation_strategy' if 'eval_strategy' doesn't work\n",
        "    learning_rate=2e-5, #look into this\n",
        "    per_device_train_batch_size=4,  # Reduce batch size for faster computation\n",
        "    per_device_eval_batch_size=4,\n",
        "    save_steps=50000,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    #logging_dir='./logs', #maybe get rid of logging\n",
        "    #logging_steps=10,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        ")\n",
        "\n",
        "# Roberta model\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=1)  # Regression output\n",
        "\n",
        "# Define Trainers\n",
        "s_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=s_train_dataset,\n",
        "    eval_dataset=s_eval_dataset,\n",
        ")\n",
        "\n",
        "b_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=b_train_dataset,\n",
        "    eval_dataset=b_eval_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a0e8857",
      "metadata": {
        "id": "1a0e8857",
        "outputId": "7aa27c79-ce40-4754-e462-9fddc0e63eb8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3182' max='74937' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3182/74937 13:15 < 4:58:59, 4.00 it/s, Epoch 0.13/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#SUBJECT\n",
        "# Train the model\n",
        "s_trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "s_eval_results = s_trainer.evaluate()\n",
        "print(f\"Subject line evaluation results: {eval_results}\")\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained('./s_trained_model')\n",
        "roberta_tokenizer.save_pretrained('./s_trained_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "788587aa",
      "metadata": {
        "id": "788587aa"
      },
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "predictions = s_trainer.predict(eval_dataset)\n",
        "\n",
        "# The predictions object contains several elements including predictions, label_ids, and metrics\n",
        "preds = predictions.predictions\n",
        "predicted_values = preds.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "751bc74a",
      "metadata": {
        "id": "751bc74a"
      },
      "outputs": [],
      "source": [
        "# Calculate regression metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "mse = mean_squared_error(s_eval_dataset['labels'], predicted_values)\n",
        "mae = mean_absolute_error(s_eval_dataset['labels'], predicted_values)\n",
        "r2 = r2_score(s_eval_dataset['labels'], predicted_values)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"Mean Absolute Error: {mae}\")\n",
        "print(f\"R-squared: {r2}\")\n",
        "\n",
        "accuracy = accuracy_score(s_eval_dataset['labels'], predicted_values)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(s_eval_dataset['labels'], predicted_values, average='binary')\n",
        "roc_auc = roc_auc_score(s_eval_dataset['labels'], predicted_values)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"ROC-AUC: {roc_auc}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}