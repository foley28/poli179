{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import glob\n",
        "import yaml\n",
        "import json\n",
        "from pandas import json_normalize"
      ],
      "metadata": {
        "id": "J3KFpBENNs1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT_ztCiNLENp"
      },
      "outputs": [],
      "source": [
        "#clones tweet data repo\n",
        "!git clone https://github.com/alexlitel/congresstweets.git\n",
        "\n",
        "#change directory\n",
        "%cd congresstweets/data\n",
        "\n",
        "#creates a list of all the json files\n",
        "json_files = glob.glob('2018*.json') + glob.glob('2019*.json') + glob.glob('2020*.json') + glob.glob('2021*.json') + glob.glob('2022*.json')\n",
        "\n",
        "#empty list that we will populate with the json dfs\n",
        "dataframes = []\n",
        "\n",
        "#loops through all the files and creates a dataframe of dataframes\n",
        "for file in json_files:\n",
        "    df = pd.read_json(file)\n",
        "\n",
        "    columns_to_keep = ['screen_name', 'text', 'time']\n",
        "    df = df[columns_to_keep]\n",
        "\n",
        "    dataframes.append(df)\n",
        "\n",
        "#concatenates all the dfs\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "#groups tweets by the twitter account\n",
        "grouped_df = combined_df.groupby('screen_name')\n",
        "\n",
        "#empty list to hold dataframes\n",
        "sampled_dataframes = []\n",
        "\n",
        "#filters out accounts with fewer than 250 tweets and samples up to 1000 tweets from each remaining account\n",
        "for account, tweets in grouped_df:\n",
        "    if len(tweets) >= 250:\n",
        "        if len(tweets) > 1000:\n",
        "            tweets = tweets.sample(n=1000, random_state=1)\n",
        "        sampled_dataframes.append(tweets)\n",
        "\n",
        "#concatenates all sampled dataframes\n",
        "sampled_combined_df = pd.concat(sampled_dataframes, ignore_index=True)\n",
        "\n",
        "print(f\"Total number of tweets after sampling and filtering: {len(sampled_combined_df)}\")\n",
        "print(f\"Number of tweets per account (should be <= 1000 and >= 250):\")\n",
        "print(sampled_combined_df.groupby('screen_name').size().describe())\n",
        "print(sampled_combined_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/alexlitel/congresstweets-automator/master/data/historical-users-filtered.json"
      ],
      "metadata": {
        "id": "z-JileASL38O",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "litel = '/content/historical-users-filtered.json'\n",
        "\n",
        "#loads it in as json\n",
        "with open(litel, 'r') as file:\n",
        "    json_data = json.load(file)\n",
        "\n",
        "#normalizes the json (flattens)\n",
        "df_histusers = json_normalize(json_data)\n",
        "\n",
        "#extracts needed, nested fields\n",
        "df_histusers[\"bioguide\"] = df_histusers['id.bioguide']\n",
        "df_histusers[\"screen_name\"] = df_histusers['accounts'].apply(lambda x: x[0][\"screen_name\"] if isinstance(x, list) and len(x) > 0 else None)\n",
        "\n",
        "#drops columns\n",
        "df_histusers = df_histusers.drop(columns=['chamber','type','id.tag','id.thomas_id','id.senate_committee_id','prev_props','state','id.house_committee_id','accounts','id.bioguide' ])\n",
        "\n",
        "print(df_histusers)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xLp9Zk58STef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/unitedstates/congress-legislators/main/legislators-social-media.yaml"
      ],
      "metadata": {
        "id": "lDzqJXPQMJRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "legis = '/content/legislators-social-media.yaml'\n",
        "with open(legis, 'r') as file:\n",
        "    yaml_data = yaml.safe_load(file)\n",
        "leg_media = pd.DataFrame(yaml_data)\n",
        "\n",
        "#lists to store extractions\n",
        "bioguide_list = []\n",
        "govtrack_list = []\n",
        "twitter_list = []\n",
        "\n",
        "#iterates over each row\n",
        "for row in yaml_data:\n",
        "    #extracts 'bioguide' and 'govtrack' from the 'id' field\n",
        "    bioguide = row['id']['bioguide']\n",
        "    govtrack = row['id'].get('govtrack')  #uses .get() to handle missing 'govtrack'(s)\n",
        "\n",
        "    #extracts 'twitter' from the 'social' field\n",
        "    twitter = row['social'].get('twitter')  #uses .get() to handle missing 'twitter'(s)\n",
        "\n",
        "    #appends data to lists\n",
        "    bioguide_list.append(bioguide)\n",
        "    govtrack_list.append(govtrack)\n",
        "    twitter_list.append(twitter)\n",
        "\n",
        "#makes a dataframe out of the lists\n",
        "df_leg_media = pd.DataFrame({\n",
        "    'bioguide': bioguide_list,\n",
        "    'govtrack': govtrack_list,\n",
        "    'twitter': twitter_list\n",
        "})\n",
        "\n",
        "print(df_leg_media)"
      ],
      "metadata": {
        "id": "ZpdCKLWFSm2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HH6Cc6YwQMus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Applied Deep Learning Poli Sci/Foley and Dorner Repo/Data/voteview.csv'\n",
        "\n",
        "voteview = pd.read_csv(file_path) #loads in the vv data\n",
        "\n",
        "voteview = voteview.drop(columns=['nominate_dim2','icpsr']) #drops columns"
      ],
      "metadata": {
        "id": "aWjaZZ7_RA8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vv_merg_hist = pd.merge(voteview, df_histusers, on='bioguide', how='left') #merges the two\n",
        "print(vv_merg_hist)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h5efRGgMRx52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_merged = pd.merge(vv_merg_hist, df_leg_media, on='bioguide', how='left') #merges the third one\n",
        "all_merged = all_merged[all_merged['bioname'] != 'TRUMP, Donald John'] #removed all NAs, i think\n",
        "all_merged = all_merged.reset_index(drop=True) #resets row numbers\n",
        "all_merged = all_merged.drop(columns=['id.govtrack','govtrack','twitter','party_code']) #drops columns\n",
        "\n",
        "print(all_merged)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dwrjaUVGVDr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_merged = pd.merge(sampled_combined_df, all_merged, on='screen_name', how='inner') #merges the three with the twitter stuff\n",
        "final_merged = final_merged.drop_duplicates(subset=['text'], keep='first') #gets rid of duplicate tweets\n",
        "final_merged = final_merged.reset_index(drop=True) #resets row numbers\n",
        "\n",
        "print(final_merged)"
      ],
      "metadata": {
        "id": "3U0de0GOw2i_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fm_path = '/content/drive/MyDrive/Applied Deep Learning Poli Sci/Foley and Dorner Repo/Data/final_merged.csv'\n",
        "#final_merged.to_csv(fm_path, index=False)"
      ],
      "metadata": {
        "id": "_XruCRljb9pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QFGinj6FN_6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_merged_path = '/content/drive/MyDrive/Applied Deep Learning Poli Sci/Foley and Dorner Repo/Data/final_merged.csv'\n",
        "final_merged = pd.read_csv(final_merged_path)"
      ],
      "metadata": {
        "id": "mnO3Rni1OHwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_merged = final_merged.dropna() #removes rows with missing values, doing this later bc I forgot to\n",
        "\n",
        "print(final_merged.sample(10, replace=False))\n",
        "print(final_merged)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "c0idi2OdPYgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fm_path = '/content/drive/MyDrive/Applied Deep Learning Poli Sci/Foley and Dorner Repo/Data/final_merged.csv'\n",
        "#final_merged.to_csv(fm_path, index=False)"
      ],
      "metadata": {
        "id": "MCEpYZZVQTRr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}