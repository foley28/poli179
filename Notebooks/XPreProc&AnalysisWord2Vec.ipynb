{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqgDcWkYrA-0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWXMUxN8qDFs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-1_JtbwqoUx"
      },
      "outputs": [],
      "source": [
        "final_merged = pd.read_csv('/content/drive/MyDrive/Applied Deep Learning Poli Sci/Foley and Dorner Repo/Data/final_merged.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnkjAiTkdsRM"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from multiprocessing import Pool\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#initializes TweetTokenizer and other preprocessing tools\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#function preprocess a singular tweet\n",
        "def preprocess_tweet(tweet):\n",
        "    tokens = tokenizer.tokenize(tweet) #tokenizes\n",
        "    tokens = [token for token in tokens if not token.startswith('http')] #removes URLs\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words] #removes stopwords and lemmatize\n",
        "    return tokens\n",
        "\n",
        "#function that preprocesses tweets in parallel\n",
        "def preprocess_tweets_parallel(tweets, num_processes=2):\n",
        "    with Pool(num_processes) as pool:\n",
        "        preprocessed_tweets = pool.map(preprocess_tweet, tweets)\n",
        "    return preprocessed_tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QPj7Y_Osj7u"
      },
      "outputs": [],
      "source": [
        "tweets = final_merged['text'].tolist()\n",
        "\n",
        "#preprocess tweets in parallel\n",
        "preprocessed_tweets = preprocess_tweets_parallel(tweets, num_processes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CwpjQ0cmttP"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "#trains the Word2Vec model\n",
        "model = Word2Vec(sentences=preprocessed_tweets, vector_size=100, window=5, min_count=100, workers=2) #these parameters can be adjusted for trial and error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWL5ra51-izf"
      },
      "outputs": [],
      "source": [
        "#model.save('/content/drive/MyDrive/Applied Deep Learning Poli Sci/Foley and Dorner Repo/Word2Vec Models/w2v100_tweets_model.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlA29ukpWBUN"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#gets the word vectors\n",
        "word_vectors = model.wv.vectors\n",
        "\n",
        "#k-means clustering\n",
        "num_clusters = 500  #adjustable\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(word_vectors)\n",
        "\n",
        "#creates a dictionary that maps words to their respective clusters\n",
        "word_to_cluster = {word: kmeans.labels_[i] for i, word in enumerate(model.wv.index_to_key)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "word_to_cluster_model = {word: int(cluster) for word, cluster in word_to_cluster.items()}\n",
        "\n",
        "with open('/content/drive/MyDrive/Applied Deep Learning Poli Sci/Foley and Dorner Repo/Models/word_to_cluster_model.json', 'w') as file:\n",
        "    json.dump(word_to_cluster_model, file)"
      ],
      "metadata": {
        "id": "nCBI3OX4iBRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QFy72eyWDEW"
      },
      "outputs": [],
      "source": [
        "#this function counts the number of clusters a tweet belongs to and then represents that tweet as a fraction of the total clusters\n",
        "def tweet_to_cluster_distribution(tweet):\n",
        "    clusters = [word_to_cluster[word] for word in tweet if word in word_to_cluster]\n",
        "    if len(clusters) == 0:\n",
        "        return np.zeros(num_clusters)  # Return zero vector if no known words\n",
        "    cluster_counts = np.bincount(clusters, minlength=num_clusters)\n",
        "    return cluster_counts / len(clusters)\n",
        "\n",
        "#runs the function on all tweets\n",
        "tweet_distributions = [tweet_to_cluster_distribution(tweet) for tweet in preprocessed_tweets]\n",
        "\n",
        "#aggregates each users tweet-distributions and represents each user as the mean of all their tweet fractions\n",
        "def aggregate_user_distributions(tweets):\n",
        "    tweet_distributions = [tweet_to_cluster_distribution(preprocess_tweet(tweet)) for tweet in tweets]\n",
        "    return np.mean(tweet_distributions, axis=0) if len(tweet_distributions) > 0 else np.zeros(num_clusters)\n",
        "\n",
        "user_distributions = final_merged.groupby('screen_name')['text'].apply(lambda x: aggregate_user_distributions(x.tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VgdxkW9gG7C"
      },
      "outputs": [],
      "source": [
        "user_distributions = user_distributions.reset_index()\n",
        "\n",
        "#dataframe with less columns\n",
        "user_ideology = final_merged[['screen_name', 'nominate_dim1']]\n",
        "\n",
        "#adds aggregate distributions to a new dataframe\n",
        "user_data = pd.merge(user_distributions, user_ideology, on='screen_name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCtmdBpT0ZgC"
      },
      "outputs": [],
      "source": [
        "#creates a binary conservative vs liberal variable\n",
        "#1 if conservative, 0 if liberal\n",
        "def threshold_function(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "bin_user = user_data.assign(conserv=user_data['nominate_dim1'].apply(threshold_function))\n",
        "print(bin_user)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRmriISQ7hul"
      },
      "outputs": [],
      "source": [
        "#creates a normalized version of the NOMINATE scores ranging from discrete values of 1-7\n",
        "user_data['normal_nom'] = ((user_data['nominate_dim1'] + 1) // (2/6)) + 1\n",
        "print(user_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a41fSZbsRCYd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#subset_bin_data = bin_user.sample(frac=.1)\n",
        "\n",
        "#creates the feature matrix\n",
        "X = np.stack(bin_user['text'].values)\n",
        "y = bin_user['conserv']\n",
        "\n",
        "#encodes labels\n",
        "y = pd.get_dummies(y).values.argmax(1)\n",
        "\n",
        "#creates training and testing sets. Testing on 20% of data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#trains the logistic regression classifier using the 'saga' solver\n",
        "#saga is more efficient but might lower our accuracy a bit\n",
        "clf = LogisticRegression(solver='saga', max_iter=1000, n_jobs=-2)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#prediction\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "#results:\n",
        "#binary accuracy: 97.3\n",
        "#1-5 accuracy: 85.4\n",
        "#1-7 accuracy: 76.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65WvrXEPEqxP"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(clf, '/content/drive/MyDrive/Applied Deep Learning Poli Sci/Foley and Dorner Repo/Models/logistic_regression_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_data.drop(columns=['text']).to_csv('/content/drive/MyDrive/Applied Deep Learning Poli Sci/Foley and Dorner Repo/Data/user_data.csv')\n",
        "bin_user.drop(columns=['text']).to_csv('/content/drive/MyDrive/Applied Deep Learning Poli Sci/Foley and Dorner Repo/Data/bin_user.csv')"
      ],
      "metadata": {
        "id": "v4nP0OrQb02D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
